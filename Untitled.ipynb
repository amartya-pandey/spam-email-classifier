{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d23f8580-5f1d-4412-a704-b4eaf7fc8229",
   "metadata": {},
   "source": [
    "# Training logic part for email classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc045e9-2e6e-4e10-9d6d-cedffa288e08",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8078d2f-dae8-409e-a6ac-9ef5ddbbb02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def load_data(filepath='email.csv'):\n",
    "    df = pd.read_csv(filepath, encoding='latin-1')\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df = df.iloc[:, :2]\n",
    "    df.columns = ['label', 'text']\n",
    "    \n",
    "    # Convert labels to binary (1 = spam, 0 = ham)\n",
    "    df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "    \n",
    "    # Clean text (remove special characters, convert to lowercase)\n",
    "    df['text'] = df['text'].apply(lambda x: re.sub(r'\\W+', ' ', x.lower()))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea59387-466d-4584-97c4-32d0bb42ad65",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ea673-777a-4a6c-9322-fc0cb809546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def train_word2vec(texts, vector_size=100, min_count=1, window=5):\n",
    "    tokenized_texts = [preprocess_text(text) for text in texts]\n",
    "    model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, min_count=min_count, window=window, workers=4)\n",
    "    return model\n",
    "\n",
    "def text_to_vector(text, word2vec_model, vector_size=100):\n",
    "    words = preprocess_text(text)\n",
    "    vectors = [word2vec_model.wv[word] for word in words if word in word2vec_model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df31c15-74ad-47b1-8f02-d6010106b85a",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fcad70-3c63-49ae-b2d3-d0aed289bbf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassifier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclassifier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_word2vec, text_to_vector\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'classifier'"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "import joblib\n",
    "from classifier.dataset_loader import load_data\n",
    "from classifier.preprocess import train_word2vec, text_to_vector\n",
    "\n",
    "# Load dataset\n",
    "df = load_data('data/email.csv')\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = train_word2vec(df['text'])\n",
    "word2vec_model.save('classifier/model/word2vec.model')\n",
    "\n",
    "# Convert text to vectors\n",
    "X = np.array([text_to_vector(text, word2vec_model) for text in df['text']])\n",
    "y = df['label'].values\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X, y)\n",
    "\n",
    "# Save trained model\n",
    "joblib.dump(nb_model, 'classifier/model/spam_classifier.pkl')\n",
    "print(\"Model training complete and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b49be4-d410-43da-b189-8f4c5e5d53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from classifier.preprocess import text_to_vector\n",
    "\n",
    "# Load trained models\n",
    "word2vec_model = Word2Vec.load('backend/classifier/model/word2vec.model')\n",
    "spam_classifier = joblib.load('backend/classifier/model/spam_classifier.pkl')\n",
    "\n",
    "def classify_email(text):\n",
    "    vector = text_to_vector(text, word2vec_model).reshape(1, -1)\n",
    "    prediction = spam_classifier.predict(vector)[0]\n",
    "    return \"Spam\" if prediction == 1 else \"Ham\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
